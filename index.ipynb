{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Tuning Neural Networks - Recap\n", "\n", "## Key Takeaways\n", "\n", "The key takeaways from this section include: \n", "\n", "* Validation and test sets are used when iteratively building deep neural networks\n", "* Like traditional machine learning models, we need to watch out for the bias variance trade-off when building deep learning models\n", "* Several regularization techniques can help us limit overfitting: L1 Regularization, L2 Regularization, Dropout Regularization, etc ...\n", "* Training of deep neural networks can be sped up by using normalized inputs\n", "* Normalized inputs can also help mitigate a common issue of vanishing or exploding gradients \n", "* Examples of alternatives for gradient descent are: RMSprop, Adam, Gradient Descent with Momentum, etc. \n", "* Hyperparameter tuning is of crucial importance when working with deep learning models, as setting the parameters right can lead to great improvements in model performance "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}